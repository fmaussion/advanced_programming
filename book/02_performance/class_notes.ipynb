{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Advanced programming: performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## The rules of optimization\n",
    "\n",
    "\n",
    "1. **Don't do it.**\n",
    "2. **(For experts only) Don't do it yet.**\n",
    "\n",
    "Inspired from famous internet programmer knowledge.\n",
    "\n",
    "### Rules of thumb\n",
    "\n",
    "<img src=\"https://imgs.xkcd.com/comics/is_it_worth_the_time_2x.png\" alt=\"xkcd\" width=\"400\"/>\n",
    "\n",
    "### If you really really want to do it anyway\n",
    "\n",
    "1. **Write tests**\n",
    "2. **Profile before optimizing**\n",
    "\n",
    "Writing tests is important to keep the code running and making sure that it works the same way after optimizing. At the very least, keep a copy of the non-optimized code and test your optimized code against it.\n",
    "\n",
    "Profiling helps to focus on what really matters. You might find the results of profiling very surprising!\n",
    "\n",
    "## Some optimisation stories from the class\n",
    "\n",
    "- vectorization instead of for-loops\n",
    "- conversion between xarray/numpy arrays etc. is expensive - avoid it if you need performance\n",
    "- writing functions instead of hard-coding every single step multiple times\n",
    "- opening the same file every time in a for-loop instead of doing it once before starting the for-loop\n",
    "- think of writing your own functions for routines you are using a lot instead of potentially slow package functions (e.g. `np.average`). See examples below.\n",
    "\n",
    "## Profiling code\n",
    "\n",
    "- default: https://docs.python.org/3/library/profile.html\n",
    "- with graphics: https://github.com/benfred/py-spy\n",
    "\n",
    "## Optimisation examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "```{important}\n",
    "\n",
    "Always follow the further rules of scientific code optimisation in order:\n",
    "\n",
    "1. Don't do it.\n",
    "2. Don't do it (yet)\n",
    "3. Test before doing it\n",
    "4. Profile before doing it\n",
    "5. Check vectorization (removing `for` loops)\n",
    "6. Remove pandas / xarray overlay\n",
    "7. Optimize numpy code\n",
    "8. Check numba or PyPy \n",
    "8. Use multiprocessing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Vectorization is awesome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "This is ALWAYS what you should check first: can you replace a for loop with numpy only arithmetics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Derivation naive\n",
    "a = np.arange(12)**2\n",
    "deriv = np.zeros(len(a))\n",
    "for i in range(len(a)):\n",
    "    if i == 0 or i == len(a) - 1:\n",
    "        continue\n",
    "    deriv[i] = (a[i+1] - a[i-1]) / 2\n",
    "deriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivation vectorized\n",
    "a = np.arange(12)**2\n",
    "deriv = np.zeros(len(a))\n",
    "deriv[1:-1] = (a[2:] - a[:-2]) / 2\n",
    "deriv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Concrete example: salem and MetPy's `interp_1d` function (useful for interpolating model levels):\n",
    "- [salem](https://github.com/fmaussion/salem/blob/0ae5885e4f466fc333a14544df851501f11102fd/salem/wrftools.py#L496-L551) uses naive interpolation but relies on multiprocessing to get speed-up\n",
    "- [MetPy](https://github.com/Unidata/MetPy/blob/8b01e1fd22304ffcd5e933cfa901a229ab8fba3f/src/metpy/interpolate/one_dimension.py#L53-L173) uses clever vectorization and is faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Back to basics: xarray -> numpy -> own functions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Because of the \"generalization overhead\" (certain numpy and xarray functions are slow because they are meant to be general and do a lot of input checks.\n",
    "\n",
    "If you know what your input looks like and have control over it, you can speed-up operations quite a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### xarray and pandas are slow on arithmetics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['data'] = np.ones(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit df**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit df.values**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "da = xr.DataArray(np.random.uniform(size=(240, 300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit da**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit da.data**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "*Question to class: why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### numpy's general functions are sometimes slower than the direct arithmetic solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.arange(1, 1000)\n",
    "w = d + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.average(d, weights=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_avg(d, wgt):\n",
    "    return (d * wgt).sum() / wgt.sum()\n",
    "\n",
    "np.testing.assert_allclose(np.average(d, weights=w), my_avg(d, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit my_avg(d, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "*Question to class: why?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "See also (similar story): https://github.com/numpy/numpy/issues/14281"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "**As for ALL cases of optimisation: please be aware of the risks! Make a careful gain / costs analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Array creation is costly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.ones(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.empty(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 100\n",
    "nx = 200\n",
    "surface_h = np.linspace(3000, 1000, nx)\n",
    "\n",
    "\n",
    "def grad_np(surface_h, dx):\n",
    "    \"\"\"Numpy computing a gradient\"\"\"\n",
    "    gradient = np.gradient(surface_h, dx)\n",
    "    gradient[[-1, 0]] = 0\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def my_grad(surface_h, dx):\n",
    "    \"\"\"Same result, not using numpy's function\"\"\"\n",
    "    gradient = np.empty(surface_h.shape)\n",
    "    gradient[1:nx-1] = (surface_h[2:] - surface_h[:nx-2])/(2*dx)\n",
    "    gradient[[-1, 0]] = 0\n",
    "    return gradient\n",
    "\n",
    "\n",
    "def my_other_grad(surface_h, dx):\n",
    "    \"\"\"Same as above, but no array creation and using concatenate\"\"\"\n",
    "    internal_grad = (surface_h[2:] - surface_h[:nx-2])/(2*dx)\n",
    "    return np.concatenate([[0], internal_grad, [0]])\n",
    "\n",
    "np.testing.assert_allclose(grad_np(surface_h, dx), my_grad(surface_h, dx))\n",
    "np.testing.assert_allclose(grad_np(surface_h, dx), my_other_grad(surface_h, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit grad_np(surface_h, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit my_grad(surface_h, dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit my_other_grad(surface_h, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Can we make it even faster? Put array creation outside of the function (optimization only available when a for loop is involved):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_container = np.zeros(surface_h.shape)\n",
    "\n",
    "def my_super_grad(surface_h, dx):\n",
    "    gradient_container[1:nx-1] = (surface_h[2:] - surface_h[:nx-2])/(2*dx)\n",
    "    return gradient_container\n",
    "\n",
    "np.testing.assert_allclose(grad_np(surface_h, dx), my_super_grad(surface_h, dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit my_super_grad(surface_h, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### But numpy is already super clever "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "The following codes do the same. It is tempting to think that the second or third examples are faster (because they use in-place operations), but it turns out that they are all exactly the same speed (within errors):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_calc(surface_h):\n",
    "    out = surface_h + 1\n",
    "    out = surface_h * out\n",
    "    out = out - 100\n",
    "    out = out**2\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_useful_optim(surface_h):\n",
    "    return (surface_h * (surface_h + 1) - 100)**2\n",
    "\n",
    "np.testing.assert_allclose(some_calc(surface_h), not_useful_optim(surface_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def also_not_useful_optim(surface_h):\n",
    "    out = surface_h + 1\n",
    "    out *= surface_h\n",
    "    out -= 100\n",
    "    return out**2\n",
    "\n",
    "np.testing.assert_allclose(some_calc(surface_h), also_not_useful_optim(surface_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit some_calc(surface_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit not_useful_optim(surface_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit also_not_useful_optim(surface_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "This is because python + numpy already knows how to compile this code to be optimized, i.e. it recognizes where the culprits are and solves them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Just in time compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "For pure numpy code with unavoidable loops (typical for numerical models with a time stepping for example), [numba](https://numba.pydata.org) is probably the best shot you have at considerably speeding up your code. We'll make an example in class.\n",
    "\n",
    "Numba is extensively used in some applications, e.g. for [cosipy](https://github.com/cryotools/cosipy). It has some drawbacks though (we discuss in class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "For some applications, [pypy](https://www.pypy.org) might be worth a shot but it requires using another python interpreter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
